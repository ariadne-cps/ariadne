/***************************************************************************
 *            nonlinear_programming.dox
 *
 *  Copyright  2009  Pieter Collins
 *
 ****************************************************************************/

/*
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Library General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 */


/*!

\file nonlinear_programming.dox
\brief Documentation on nonlinear programming


\page nonlinear_programming_page NonLinear Programming

This page describes the theory of nonlinear programming and algorithms for the rigorous numerical solution of nonlinear optimisation problems.
For details on how this is implemented in %Ariadne, see the \ref OptimisationSubModule documentation


\section nonlinear_optimisation Optimality Conditions

We first give the theory of nonlinear programming, including first- and second-order conditions for a local optimum.


\subsection standard_nonlinear_optimisation Standard constrained nonlinear programming problem

Consider the nonlinear programming problem
\f[ \label{eq:nlp} \boxed{ \min f(x) \text{ s.t. } g_j(x)\leq 0,\ j=1,\ldots,l; \ h_k(x) = 0,\ k=1,\ldots,m . } \f]
Applying a barrier function for the inequality constraints and a penalty function for the equality constraints yields the unconstrained minimisation of
\f[ f(x) - \mu  \sum_{j=1}^{l} \log(-g_j(x)) + \frac{1}{2\nu} \sum_{k=1}^{m} (h_k(x) )^2 . \f]
Differentiating with respect to \f$x\f$ yields
\f[ \nabla_{\!i\,} f(x) - \mu  \sum_{j=1}^{l} \frac{\nabla_{\!i\,}g_j(x)}{g_j(x)} + \frac{1}{\nu} \sum_{k=1}^{m} h_k(x)\nabla_{\!i\,}h_k(x) = 0. \f]
Setting \f$y_j = -\mu/g_j(x)\f$ and \f$z_k = h_k(x)/\nu\f$ yields
\f[ \nabla_{\!i\,} f(x) +  \sum_{j=1}^{l} y_j \nabla_{\!i\,}g_j(x) + \sum_{k=1}^{m} z_k\,\nabla_{\!i\,}h_k(x) . \f]
Combining these equations yields
\f[ \begin{gathered} \nabla_{\!i\,} f(x) + \sum_{j=1}^{l} y_j \nabla_{\!i\,} g_j(x) + \sum_{k=1}^{m} z_k \nabla_{\!i\,} h_k(x) = 0; \\ y_j g_j(x) + \mu = 0; \\ h_k(x) + \nu z_k = 0; \\ g_j(x) \leq 0; \ y_j \geq 0 . \end{gathered} \f]
Taking \f$\nu\to0\f$ in these equations yields the <em>central path</em> for the problem, which is a relaxation of the optimality conditions
\f[ \boxed{ \begin{gathered} \nabla_{\!i\,} f(x) + \sum_{j=1}^{l} y_j \nabla_{\!i\,} g_j(x) + \sum_{k=1}^{m} z_k \nabla_{\!i\,} h_k(x) = 0; \\ y_j g_j(x) + \mu = 0; \\ h_k(x) = 0; \\ y_j \gt 0; \ g_j(x) \lt 0 . \end{gathered} } \f]
Taking \f$\mu\to0\f$ yields the standard Karush-Kuhn-Tucker (KKT) conditions for optimality:
\f[ \boxed{ \begin{gathered} \nabla_{\!i\,} f(x) + \sum_{j=1}^{l} y_j \nabla_{\!i\,} g_j(x) + \sum_{k=1}^{m} z_k \nabla_{\!i\,} h_k(x) = 0; \\ y_j g_j(x) = 0; \\ h_k(x) = 0; \\ g_j(x) \leq 0; \ y_j \geq 0 . \end{gathered} } \f]
The Karush-Kuhn-Tucker conditions are necessary conditions for a <em>regular</em> local optimum i.e. one for which that gradients \f$\nabla h_k(x)\f$ of the equality constraints, and \f$\nabla g_j(x)\f$ for the <em>active</em> inequality constraints, are a linearly independent set of vectors.

The standard (Fritz) John necessary conditions for optimality are
\f[ \boxed{ \begin{gathered} r \nabla_{\!i\,} f(x) + \sum_{j=1}^{l} y_j \nabla_{\!i\,} g_j(x) + \sum_{k=1}^{m} z_k \nabla_{\!i\,} h_k(x) = 0; \\ r + \sum_{j=1}^{l} y_j + \sum_{k=1}^{m} z_k^2 = 1; \\ y_j g_j(x)=0; \\ h_k(x)=0; \\ r\geq 0; \ y_j \geq 0; \ g_j(x) \leq 0 . \end{gathered} } \f]
The John conditions are necessary conditions for any local optimum (assuming differentiability of \f$f,g_j,h_k\f$).

Introducing slack variables \f$w_j=-g_j(x)\f$ gives the central path
\f[ \boxed{ \begin{gathered} \nabla_{\!i\,} f(x) + \sum_{j=1}^{l} y_j \nabla_{\!i\,} g_j(x) + \sum_{k=1}^{m} z_k \nabla_{\!i\,} h_k(x) = 0; \\ w_j y_j - \mu = 0; \\ g_j(x)+w_j = 0 \\ h_k(x)=0; \\ r\geq 0; \ w_j > 0; \ y_j > 0. \end{gathered} } \f]
and John conditions
\f[ \boxed{ \begin{gathered} r \nabla_{\!i\,} f(x) + \sum_{j=1}^{l} y_j \nabla_{\!i\,} g_j(x) + \sum_{k=1}^{m} z_k \nabla_{\!i\,} h_k(x) = 0; \\ r + \sum_{j=1}^{l} y_j + \sum_{k=1}^{m} z_k^2 = 1 \\ w_j y_j =0; \\ g_j(x)+w_j = 0 \\ h(x)=0; \\ r\geq 0; \ w_j\geq 0; \ y_j \geq 0. \end{gathered} } \f]


\note Taking \f$f(x) = cx\f$, \f$g(x)=-x\f$ and \f$h(x)=Ax-b\f$ yields the primal linear programming problem \f$\min cx \mid Ax=b,\ x\geq 0\f$.
Taking \f$f(y)=yb\f$, \f$g(y) = yA-c\f$ yields the dual linear programming problem \f$\max yb \mid yA\leq c\f$.
This means that the standard \em primal nonlinear programming problem with only affine inequality constraints more closely resembles the \em dual linear programming problem.
Despite this, we shall use \f$x\f$ for the primal variables of the standard primal nonlinear programming problem.

Necessary conditions and sufficient conditions for local optimality require second derivatives.
Let
\f[ \boxed{ \begin{gathered} Q = \nabla^2{f}(x) + \sum_{j=1}^{l} y_j \nabla^2{g_j}(x) + \sum_{k=1}^{m} z_k \nabla^2{h_k}(x); \\  A = \nabla{g}(x)^T = Dg(x); \  B = \nabla{h}(x)^T=Dh(x); \ c = \nabla{f}(x); \\ w=-g(x); \ W = \mathrm{diag}{w};\ Y=\mathrm{diag}(y) . \end{gathered} } \f]
Then if \f$x\f$ is a local minimum, we must have
\f[ \boxed{ \forall s \in \R^n,\; Y A s=0 \wedge B s=0 \implies s^T Q s \geq 0 . } \f]
We say constraint \f$g_j \leq 0\f$ is <em>active</em> at \f$x\f$ if \f$g_j(x)=0\f$.
The optimality condition restricts the vectors \f$s\f$ by \f$\nabla{g_j}(x)\cdot s = 0\f$ whenever \f$g_j \leq 0\f$ is an active constraint, but places no restrictions if the constraint is inactive, since \f$y_j \nabla{g_j}(x)\cdot s\f$ is always \f$0\f$ since \f$y_j=0\f$.

Sufficient conditions for a local minimum require both
\f[ \boxed{ \begin{gathered} \forall j,\; g_j(x) < 0 \vee y_j > 0; \\ \forall s\in\R^n\setminus\{0\},\; Y As=0 \wedge Bs=0 \implies s^T Q s > 0 . \end{gathered} } \f]

A <em>Lagrangian function </em> for this problem is
\f[ \boxed{\displaystyle L(x,y,z) = f(x) + \sum_{j=1}^{l} y_j g_j(x) + \sum_{k=1}^{m} z_k h_k(x) . } \f]
The standard Karush-Kuhn-Tucker conditions for optimality are obtained by setting the partial derivatives of \f$L\f$ to zero, with the inequality constraints \f$y_j,g_j(x)\geq0\f$ allowing for the relaxation \f$y_j=0 \vee g_j(x)=0\f$.
The matrix \f$Q\f$ is given by \f$Q=\nabla^2_{\!x\,}L(x,y,z)\f$.

\subsubsection nonlinear_inequality_constraints Inequality constrained problem

Consider the problem
\f[ \max f(x) \text{ s.t. } g(x) \leq 0 . \f]
The central path is defined by
\f[ \boxed{ \begin{gathered}  \nabla f(x) + y \cdot \nabla g(x) = 0; \\ y \circ g(x) = - \mu; \\ y \geq 0,\ g(x) \leq 0. \end{gathered} } \f]
Taking \f$\mu\to 0\f$, we obtain the standard Karush-Kuhn-Tucker conditions.
\f[ \boxed{ \begin{gathered}  \nabla f(x) + y \cdot \nabla g(x) = 0; \\ y \circ g(x) = 0; \\ y \geq 0,\ g(x) \leq 0. \end{gathered} } \f]


\subsubsection nonlinear_equality_constraints Equality constrained problem

Consider the problem
\f[ \max f(x) \text{ s.t. } h(x) = 0 . \f]
The Karush-Kuhn-Tucker optimality conditions are
\f[ \boxed{ \begin{gathered}  \nabla f(x) + z \cdot \nabla h(x) = 0; \\ h(x) = 0. \end{gathered} } \f]


\subsubsection nonlinear_bounded_constraints Nonlinear programming problem with bounded constraints

Consider the problem
\f[ \min f(x) \text{ s.t. } \unl{c} \leq g(x) \leq \ovl{c}. \f]

Considering the smooth constraint \f$(g(x)-\unl{c})(g(x)-\ovl{c})\leq0\f$, we obtain the central path equations
\f[ \begin{gathered} \textstyle \nabla f(x) + y' (2 g(x) - \unl{c}-\ovl{c}) \nabla g(x) = 0; \\ y' (g(x)-\unl{c})(g(x)-\ovl{c}) + \mu = 0. \end{gathered} \f]
Replacing \f$ y' (2 g(x) - \unl{c} - \ovl{c})\f$ with \f$y\f$, we obtain
\f[ \begin{gathered} \nabla f(x) + y \nabla g(x) = 0; \\ y (g(x)-\unl{c})(g(x)-\ovl{c}) / (2 g(x) - \unl{c} - \ovl{c}) + \mu = 0. \end{gathered} \f]
Rearranging the second formula gives
\f[ \boxed{ \begin{gathered} \nabla f(x) + y \cdot \nabla g(x) = 0; \\ (g(x)-\unl{c})(\ovl{c}-g(x)) y - (2 g(x) -\unl{c}-\ovl{c}) \mu = 0 . \end{gathered} } \f]

Alternatively, considering \f$g(x)-\unl{c}\geq0\f$ and \f$\ovl{c} - g(x) \geq0\f$, we obtain the central path
\f[ \begin{gathered} \nabla f(x) + \ovl{y} \cdot \nabla g(x) - \unl{y} \cdot \nabla g(x) = 0; \\ \unl{y} (g(x)-\unl{c}) = \mu; \\ \ovl{y} (\ovl{c} - g(x)) = \mu \end{gathered} \f]
Set \f$y = \ovl{y}-\unl{y}\f$. Then we have \f$\nabla{f}+y \cdot \nabla{g}(x) = 0\f$ and
\f[ y = \mu \biggl( \frac{1}{\ovl{c}-g(x)} - \frac{1}{g(x)-\unl{c}}\biggr) = \frac{(2 g(x) -\unl{c}-\ovl{c}) \mu}{(g(x)-\unl{c})(\ovl{c}-g(x))} . \f]
Rearranging again gives
\f[ (g(x)-\unl{c})(\ovl{c}-g(x)) y - (2 g(x) -\unl{c}-\ovl{c}) \mu = 0\f]
If \f$\ovl{c}=-\unl{c}=c\f$, this simplifies to \f[ (c^2-g(x)^2) y - 2 g(x) \mu = 0 . \f]

Note that for \f$\unl{c}\leq g(x)\leq\ovl{c}\f$ we have \f$|2g(x)-\unl{c}-\ovl{c}|\leq \ovl{c}-\unl{c}\f$, so \f$ \unl{y}+\ovl{y} \geq \bigl|\ovl{y}-\unl{y}\bigr| . \f$


\subsubsection nonlinear_state_constraints State constraints

Taking state constraints \f$\unl{d}\leq x\leq\ovl{d}\f$, we obtain the central path
\f[ \boxed{ \begin{gathered} \nabla f(x) + z = 0; \\ (\ovl{d}-x)(x-\unl{d})z + (2x-\unl{d}-\ovl{d}) \mu = 0 . \end{gathered} } \f]
If \f$\ovl{d}=-\unl{d}=d\f$, this simplifies to \f$ (d^2-x^2) z - 2 x \mu = 0 . \f$



\subsection general_nonlinear_optimisation General Optimisation Problem

In %Ariadne, we need to problems with a more general form of the constraints including bounds on the variables and constraints with upper and lower bounds.
We henceforth consider the generalised problem:
\f[ \boxed{\min f(x) \text{ s.t. } \unl{c}_j \leq g_j(x)\leq  \ovl{c}_j, \ j=1,\ldots,m; \  \unl{d}_i \leq x_i\leq  \ovl{d}_i,\ i=1,\ldots,n. } \f]
We make the constraints on \f$g\f$ into equality constraints by introducing variables \f${w}_i\f$
\f[ g_j(x) - w_j = 0, \ \unl{c}_j \leq w_j \leq \ovl{c}_j,\ j=1,\ldots,m. \f]
We add a barrier and a penalty function, and attempt to minimise the function
\f[ f(x) - \mu \sum_{j=1}^{m} \bigl(\log(w_j-\unl{c}_j) + \log(\ovl{c}_j-w_j) \bigr) - \mu \sum_{i=1}^{n} \bigl( \log(x_i-\unl{d}_i) + \log(\ovl{d}_i-x_i) \bigr)  + \frac{1}{2\nu} \sum_{j=1}^{m} (g_j(x)-w_j)^2 . \f]
Differentiating with respect to \f$w\f$ and \f$x\f$ gives
\f[ \begin{gathered} \mu \Bigl(\frac{1}{\ovl{c}_j-w_j} - \frac{1}{w_j-\unl{c}_j} \Bigr) - \frac{1}{\nu} (g_j(x)-w_j) = 0; \\ \nabla_{\!i\,}f(x) + \mu \Bigl( \frac{1}{\ovl{d}_i-x_i} - \frac{1}{x_i-\unl{d}_i} \Bigr) + \frac{1}{\nu} \sum_{j=1}^{m} (g_j(x)-w_j) \nabla_{\!i\,}g_j(x) = 0.  \end{gathered} \f]
Introducing Lagrange multipliers \f$y_j = (g_j(x)-w_j)/\nu\f$ gives
\f[ \begin{gathered} g_j(x)-w_j - \nu y_j  = 0; \\ \nabla_{\!i\,}f(x) + \mu \Bigl( \frac{1}{\ovl{d}_i-x_i} - \frac{1}{x_i-\unl{d}_i} \Bigr) + \sum_{j=1}^{m} y_j \nabla_{\!i\,}g_j(x) = 0; \\  \mu \Bigl( \frac{1}{\ovl{c}_j-w_j} - \frac{1}{w_j-\unl{c}_j} \Bigr) -  y_j = 0.  \end{gathered} \f]
Setting \f$z_i=\mu \bigl( 1/(\ovl{d}_i-x_i) - 1/(x_i-\unl{d}_i) \bigr)\f$ and taking \f$\nu=0\f$ yields the <em>central path conditions</em>
\f[ \boxed{ \displaystyle \begin{gathered}   g_j(x)-w_j = 0 ; \\ \nabla_{\!i\,}f(x) + \sum_{j=1}^{m} y_j \nabla_{\!i\,}g_j(x) + z_i = 0; \\ \mu \Bigl( \frac{1}{\ovl{c}_j-w_j} - \frac{1}{w_j-\unl{c}_j} \Bigr) -  y_j = 0 ; \\ \mu \Bigl( \frac{1}{\ovl{d}_i-x_i} - \frac{1}{x_i-\unl{d}_i} \Bigr) - z_i = 0; \\ \unl{c}_j < w_j < \ovl{c}_j; \\ \unl{d}_i < x_i < \ovl{d}_i. \end{gathered} } \f]
Equivalently
\f[ \boxed{ \displaystyle \begin{gathered}  g_j(x)-w_j = 0; \\  \nabla_{\!i\,}f(x) - \sum_{j=1}^{m} y_j \nabla_{\!i\,}g_j(x) - z_i = 0; \\ (w_j-\unl{c}_j)(\ovl{c}_j-w_j)y_j -  \mu (2w_j - \unl{c}_j - \ovl{c}_j) = 0; \\ (x_i-\unl{d}_i)(\ovl{d}_i-x_i)z_i - \mu (2x_i - \unl{d}_i - \ovl{d}_i) = 0; \\ \unl{c}_j \leq w_j \leq \ovl{c}_j; \\ \unl{d}_i < x_i < \ovl{d}_i .  \end{gathered} } \f]

If \f$ \unl{c}_j < \ovl{c}_j \f$, then we must have \f$\unl{c}_j < w_j < \ovl{c}_j\f$, since if \f$w_j\in\{\unl{c}_j,\ovl{c}_j\}\f$ we have \f$(w_j-\unl{c}_j)(\ovl{c}_j-w_j)y_j=0\f$ but \f$\mu (\unl{c}_j + \ovl{c}_j - 2w_j)=\pm\mu(\ovl{c}_j-\unl{c}_j)\neq0\f$, a contradiction.

If \f$ \unl{c}_j = \ovl{c}_j \f$ have common value \f$c_j\f$, then \f$ -(w_j - c_j)^2 y_j - 2 \mu(w_j-c_j)=0 \f$. Hence \f$w_j=c_j\f$ and there are no further conditions on \f$y_j\f$.
The equation \f$(w_j-\unl{c}_j)(\ovl{c}_j-w_j)y_j -  \mu (2w_j - \unl{c}_j - \ovl{c}_j) = 0\f$ is therefore replaced by \f$w_j-c_j = 0\f$ for this \f$j\f$.

If \f$\unl{c}_j=0\f$ and \f$\ovl{c}_j=\infty\f$, then \f$ \mu ( 1/\infty - 1/w_j ) - y_j = 0  \f$ so \f$ w_j \,y_j + \mu = 0  \f$ and \f$y_j<0\f$.
If \f$\unl{c}_j=-\infty\f$ and \f$\ovl{c}_j=0\f$, then \f$ \mu ( 1/w_j - 1/\infty ) - y_j = 0  \f$ so \f$ w_j \,y_j - \mu = 0 \f$ and \f$y_j>0\f$.

We therefore have the following special cases for the central path:
\f[ \boxed{ \displaystyle \begin{gathered} \unl{c}_j < \ovl{c}_j \implies \unl{c}_j < w_j < \ovl{c}_j ; \\ \unl{c}_j = \ovl{c}_j = c_j \implies w_j = c_j; \\ \unl{c}_j = 0 \wedge \ovl{c}_j = \infty \implies w_j y_j + \mu = 0 \wedge y_j<0 ; \\ \unl{c}_j = -\infty \wedge \ovl{c}_j = 0 \implies w_j y_j - \mu = 0 \wedge y_j>0 . \end{gathered} } \f]


The Karush optimality conditions are obtained by taking \f$\mu\to 0\f$:
\f[ \boxed{ \displaystyle \begin{gathered}  g_j(x)-w_j = 0; \\ \nabla_{\!i\,}f(x) + \sum_{j=1}^{m} y_j \nabla_{\!i\,}g_j(x) + z_i = 0; \\ (w_j-\unl{c}_j)(\ovl{c}_j-w_j)y_j = 0; \\ (x_i-\unl{d}_i)(\ovl{d}_i-x_i)z_i = 0; \\ \unl{c}_j \leq w_j \leq \ovl{c}_j; \\  \unl{d}_i \leq x_i \leq \ovl{d}_i .  \end{gathered} } . \f]

Note that we can separate out the lower and upper terms by taking
\f$ \unl{y}_j = \mu/(w_j-\unl{c}_j), \ \ovl{y}_j = \mu/(\ovl{c}_j-w_j),\ \unl{z}_i = \mu/(x_i-\unl{d}_i), \ \ovl{z}_i = \mu/(\ovl{d}_i-x_i)\f$ to obtain the central path
\f[ \boxed{\begin{gathered}
       g_j(x) - w_j = 0. \\
       \nabla_{\!i\,}f(x)+\sum_{j=1}^{m} (\ovl{y}_j - \unl{y}_j) \nabla_{\!i\,} g_j(x) + (\ovl{z}_i - \unl{z}_i) = 0 \\
       ({w}_j-\unl{c}_j)\unl{y}_j = \mu; \quad (\ovl{c}_j-w_j)\ovl{y}_j = \mu; \\
       (x_i-\unl{d}_i)\unl{z}_i = \mu; \quad (\ovl{d}_i-x_i)\ovl{z}_i = \mu; \\
       \unl{c}_j \leq w_j \leq \ovl{c}_j; \quad \unl{y}_j,\;\ovl{y}_j > 0; \\
       \unl{d}_i < x_i < \ovl{d}_i; \quad \unl{z}_i,\;\ovl{z}_i >0 .
    \end{gathered} }
\f]

We can simplify the formulation by dropping explicit state constraints, obtaining:
\f[ \boxed{\min f(x) \text{ s.t. } \unl{c}_j \leq g_j(x)\leq  \ovl{c}_j, \ j=1,\ldots,m. } \f]
The central patch conditions are then
\f[ \boxed{ \displaystyle \begin{gathered}  \nabla_{\!i\,}f(x) + \sum_{j=1}^{m} y_j \nabla_{\!i\,}g_j(x) = 0; \\ (g_j(x)-\unl{c}_j)(\ovl{c}_j-g_j(x))y_j -  \mu (2g_j(x) - \unl{c}_j - \ovl{c}_j) = 0; \\ \unl{c}_j \leq g_j(x) \leq \ovl{c}_j .  \end{gathered} } \f]


\subsubsection nonlinear_optimisation_as_linear The linear case

If \f$f(x) = c^Tx\f$, \f$g(x)=Ax-b\f$, \f$\unl{c}=\ovl{c}=0\f$, \f$\unl{d}=0\f$ and \f$\ovl{d}=\infty\f$, then we have the primal linear programming problem
\f[ \min c^Tx \text{ s.t. } Ax=b,\ x\geq 0. \f]
We obtain the central path equations
\f[  x \circ z = \mu; \quad A^Ty + z = c; \quad Ax = b,\f]
which are a relaxed version of the standard conditions for linear programming.

Dually, if \f$f(x)=-b^Tx\f$, \f$g(x)=A^Tx\f$, \f$\unl{c}=-\infty\f$, \f$\ovl{c}=c\f$, \f$\unl{d}=-\infty\f$ and \f$\ovl{d}=+\infty\f$, then we have the dual linear programming problem
\f[ \max b^Tx \text{ s.t. } A^Tx\leq c . \f]
We obtain the central path equations
\f[  w_j\,y_j = \mu; \quad Ay = b; \quad A^T x+w = c . \f]
This is a relaxed version of the standard conditions for the dual linear programming with \f$w\f$ instead of \f$z\f$ for the slack variables, and \f$x\f$ and \f$y\f$ interchanged.


\subsection general_nonlinear_feasibility General feasibility problem

Consider the generalised feasibility problem:
\f[ \boxed{ \unl{c}_j \leq g_j(x)\leq  \ovl{c}_j, \ j=1,\ldots,m; \  \unl{d}_i \leq x_i\leq  \ovl{d}_i;\ i=1,\ldots,n. } \f]
where \f$-\infty < \unl{d}_i < \ovl{d}_i < +\infty\f$.

Maximising the <em>feasibility</em>
\f[ \mu \sum_{j=1}^{m} \bigl(\log(w_j-\unl{c}_j) + \log(\ovl{c}_j-w_j) \bigr) + \mu \sum_{i=1}^{n} \bigl( \log(x_i-\unl{d}_i) + \log(\ovl{d}_i-x_i) \bigr) \f]
given the constraints \f$g_j(x)=w_j\f$ is equivalent to optimisation taking \f$f(x)=0\f$ and adding a barrier function. This yields the following central path
\f[ \boxed{\begin{gathered}
       g_j(x) - w_j = 0; \\
       \sum_{j=1}^{m} y_j \nabla_{\!i\,} g_j(x) + z_i = 0 ; \\
       (w_j-\unl{c}_j)(\ovl{c}_j-w_j)y_j - (2w_j-\unl{c}_j-\ovl{c}_j)\mu = 0 ; \\
       (x_i-\unl{d}_i)(\ovl{d}_i-x_i)z_i - (2x_i-\unl{d}_i-\ovl{d}_i)\mu = 0 ; \\
       \unl{c}_j \leq w_j \leq \ovl{c}_j; \\ \unl{d}_i < x_i < \ovl{d}_i
\end{gathered} } . \f]
The <em>most feasible point</em> is given by
\f[ \boxed{ \displaystyle \begin{gathered} g_j(x)-w_j = 0; \\ \sum_{j=1}^{m} y_j \nabla_{\!i\,}g_j(x) + z_i = 0; \\ (w_j-\unl{c}_j)(\ovl{c}_j-w_j)y_j = 0; \\ (x_i-\unl{d}_i)(\ovl{d}_i-x_i)z_i = 0; \\ \unl{c}_j \leq w_j \leq \ovl{c}_j; \\ \unl{d}_i < x_i < \ovl{d}_i.  \end{gathered} } \f]

The Lagrangian function for feasibility is defined as
\f[ \boxed{ \displaystyle L(w,x,y,\mu) = f(x) + \mu \sum_{j=1}^{m} \bigl( \log(w_j-\unl{c}_j)+\log(\ovl{c}_j-w_j) \bigr) + \mu \sum_{i=1}^{n} \bigl( \log(x_i-\unl{d}_i)+\log(\ovl{d}_i-x_i) \bigr) + \sum_{j=1}^{m} y_j(g_j(x)-w_j) } \f]

An alternative approach is to maximise the minimal slack among \f$g_j(x)-\unl{c}_j\f$ and \f$\ovl{c}_j-g_j(x)\f$.
We introduce a new variable \f$t\f$, change constraints to \f$g_j(x)-t-\unl{c}_j\geq0\f$ and \f$\ovl{c}_j-t-g_j(x)\geq 0\f$, and consider the objective function \f$t\f$.






\subsubsection nonlinear_inequality_feasibility Inequality constrained feasibility problems

Consider the feasibility problem
\f[ g_j(x) \leq 0, \ j=1,\ldots,m; \ \ \unl{d}_i \leq x_i\leq  \ovl{d}_i,\ i=1,\ldots,n. \f]
We can re-cast this as an optimisation problem by introducing an extra variable \f$t\f$ and taking
\f[ \max t \ \text{ s.t. } \ g_j({x})+t\leq 0,\ j=1,\ldots,m;\ \ \unl{d}_i \leq x_i\leq  \ovl{d}_i,\ i=1,\ldots,n. \f]
The central path equations with dual variables \f$y,z\f$ and slack variables \f$w\f$ are
\f[ \begin{gathered} \textstyle \sum_{i=1}^{m} y_j \nabla_{\!i\,} g_j({x}) + z_i = 0; \\ \sum_{j=1}^{n} y_j =1; \\ g_j({x}) + t + {w}_j = 0; \\ (x_i-\unl{d}_i)(\ovl{d}-x_i)z_i - (2x_i-\unl{d}_i-\ovl{d}_i) \mu = 0; \\ {w}_jy_j= \mu; \\ {w}_j,y_j\geq 0,\ j=1,\ldots,m . \end{gathered} \f]



\section approximate_nonlinear_optimisation Approximate methods for nonlinear programming


We now consider the general nonlinear programming problem
\f[ \min f(x) \text{ s.t. } \unl{d}_i \leq x_i \leq \ovl{d}_i ,\ \unl{c}_j \leq g_j(x) \leq \ovl{c}_j . \f]
We assume a domain \f$D\f$ with nonempty interior, so \f$\unl{d}_i < \ovl{d}_i\f$, but allow for the other constraints to be equality constraints \f$\unl{c}_j=\ovl{c}_j\f$ or unbounded \f$\unl{c}_j=-\infty\f$ or \f$\ovl{c}_j=+\infty\f$. If \f$D\f$ is bounded, then the codomain \f$C\f$ can be bounded by intersecting with \f$[g(D)]\f$.

We may set \f$w_j=g_j(x)\f$, so we can write \f$\unl{c} \leq w_j \leq \ovl{c}_j\f$ and \f$g_j(x)-w_j=0\f$.
However, we treat bounded, unbounded and equality constraints slightly differently.

\subsection newton_methods_for_central_path Newton methods for solving the central path equations

We consider the central path conditions in the following form:
\f[ \begin{gathered} s_j(w_j,y_j,\mu) = 0; \\ \nabla_{\!i\,} f(x) + \sum_{j=1}^{m} y_j \nabla_{\!i\,} g(x) + z = 0; \\ g_j(x) - w_j = 0; \\ t_i(x_i,z_i,\mu) = 0 , \end{gathered} \f]
where \f$s_j(w_j,y_j,\mu)\f$ depends on the bounds \f$\unl{c}_j,\ovl{c}_j\f$, and \f$t_i(x,z,\mu)\f$ on the bounds \f$\unl{d}_i\leq x_i \leq \ovl{d}_i\f$.

\subsubsection newton_complementarity_equations  Newton equations for complementarity conditions

A full Newton step for the equation \f$h(x)=0\f$ is given by \f$x'=x+\Delta{x}\f$ where \f$Dh(x)\Delta{x}=-h(x)\f$, and a partial Newton step is given by \f$x'=x+\alpha\Delta{x}\f$ for some \f$\alpha\in(0\!:\!1]\f$.
For a given nonlinear equation \f$h_i(x)=0\f$, this gives a single linear equation \f$\nabla{h_i}(x)\cdot\Delta{x}=-h_i(x)\f$.
For the constraint \f$s(w,y,\mu)=0\f$, we therefore have
\f[ s_{,w}(w,y,\mu) \Delta{w} + s_{,y}(w,y,\mu) \Delta{y} = - s(w,y,\mu) . \f]
We may further scale these equations by a function of \f$w,y\f$ to yield a simpler form.

For the constraints \f$\unl{c}\leq w \leq \ovl{c}\f$ with \f$-\infty < \unl{c} < \ovl{c} < +\infty\f$, there are many equivalent forms for \f$s\f$.

The form
\f[s(w,y,\mu) = (w-\unl{c})(\ovl{c}-w)y - (2w-\unl{c}-\ovl{c})\mu = 0 \f]
is useful since it does not involve division by \f$2w-\unl{c}-\ovl{c}\f$, which may be singular. It gives rise to the update
\f[ -\bigl((2w-\unl{c}-\ovl{c})y + 2\mu\bigr) \Delta{w} + (w-\unl{c})(\ovl{c}-w)\Delta{y} = -\bigl((w-\unl{c})(\ovl{c}-w)y - (2w-\unl{c}-\ovl{c})\mu\bigr) \f]
which is equivalent to
\f[ \bigl(((\ovl{c}-w)y-\mu) + ((w-\unl{c})(-y) - \mu)\bigr) \Delta{w} + (w-\unl{c})(\ovl{c}-w)\Delta{y} = -\bigl((w-\unl{c})(\ovl{c}-w)y - (w-\unl{c}) + (\ovl{c}-w))\mu\bigr) . \f]
As a special case, constraint \f$-c\leq g(x) \leq c\f$ has the form
\f[ s(w,y,\mu) = (c^2 - w^2)y - 2 w \mu \f]
which gives rise to
\f[ -2(wy+\mu) \Delta{w} + (c^2-w^2)\Delta{y} = - \bigl( (c^2 - w^2)y - 2 w \mu \bigr).  \f]
so
\f[ \Delta{y}  = \frac{2 (wy + \mu) \Delta{w} - \bigl( (c^2 - w^2)y - 2 w \mu \bigr)}{c^2-w^2}
= \frac{2 (wy + \mu) \Delta{w}-2w\mu}{c^2-w^2} - y . \f]

The form
\f[s(w,y,\mu) = \frac{1}{1/(\ovl{c}-w)-1/(w-\unl{c})}\,y-\mu = \frac{(w-\unl{c})(\ovl{c}-w)}{(2w-\unl{c}-\ovl{c})}\,y - \mu\f]
is useful since the coefficients of \f$\Delta{w},\Delta{y}\f$ do not involve \f$\mu\f$.
It gives rise to the update \f[\bigl(-1-2(w-\unl{c})(\ovl{c}-w)/(2w-\unl{c}-\ovl{c})^2\bigr)y \Delta{w} + (w-\unl{c})(\ovl{c}-w)/(2w-\unl{c}-\ovl{c}) \Delta{y} = - \bigl((w-\unl{c})(\ovl{c}-w)/(2w-\unl{c}-\ovl{c})\;y - \mu\bigr) . \f]
Multiplying through by \f$(2w-\unl{c}-\ovl{c})^2\f$ to avoid division by a potentially zero quantity gives
\f[ -\bigl((w-\unl{c})^2+(\ovl{c}-w))^2\bigr)y \Delta{w} + (w-\unl{c})(\ovl{c}-w)(2w-\unl{c}-\ovl{c}) \Delta{y} = - \bigl((w-\unl{c})(\ovl{c}-w)\;y - \mu(2w-\unl{c}-\ovl{c})\bigr)(2w-\unl{c}-\ovl{c}) . \f]

The values of \f$s(w,y,\mu)\f$ are given below for various constraints involving \f$w=g(x)\f$:
 - \f$g(x)=c : s(w,y,\mu) = w-c; \ \ \Delta{w}=-(w-c). \f$
 - \f$g(x)\geq \unl{c} : s(w,y,\mu) = (w-\unl{c}) y - \mu; \ \ y\Delta{w}+(w-\unl{c})\Delta{y}=-((w-\unl{c}) y - \mu) . \f$
 - \f$g(x)\leq \ovl{c} : s(w,y,\mu) = (\ovl{c}-w) y - \mu; \ \ -y\Delta{w}+(\ovl{c}-w)\Delta{y}=-((\ovl{c}-w) y - \mu) \f$
 \par
 - \f$|g(x)|\leq c : s(w,y,\mu) = (c^2-w^2)y - 2w\mu; \ \partial s/\partial w = -2(wy+\mu); \ \ -2(wy+\mu)\Delta{w} - w^2\Delta{y} = -\bigl((c^2-w^2)y - 2w\mu \bigr); \f$
 - \f$\unl{c} \leq g(x) \leq \ovl{c}: s(w,y,\mu) = (\ovl{c}-w)(w-\unl{c})y -(2w-\unl{c}-\ovl{c})\mu; \ \ \big((2w-\unl{c}-\ovl{c})y - 2\mu\bigr)\Delta{w}+(\ovl{c}-w)(w-\unl{c})\Delta{y}=-\bigl((\ovl{c}-w)(w-\unl{c})y -(2w-\unl{c}-\ovl{c})\mu\bigr).\f$
 - \f$\unl{c} \leq g(x) \leq \ovl{c}: s(w,y,\mu) = (w-\unl{c})(\ovl{c}-w)/(2w-\unl{c}-\ovl{c})\;y-\mu; \ \  -\bigl((w-\unl{c})^2+(\ovl{c}-w))^2\bigr)y \Delta{w} + (w-\unl{c})(\ovl{c}-w)(2w-\unl{c}-\ovl{c}) \Delta{y} = - \bigl((w-\unl{c})(\ovl{c}-w)\;y - \mu(2w-\unl{c}-\ovl{c})\bigr)(2w-\unl{c}-\ovl{c}) . \f$.

Note that in the case of an equality constraint \f$g(x)=c\f$, we initialise \f$w\f$ to \f$c\f$, so \f$\Delta{w}=0\f$.

\subsubsection newton_central_path_solution Solving the Newton equations for the central path

The Jacobian matrix for the central path conditions is given by
\f[ \begin{pmatrix} \nabla_{\!w\,}s(w,y,\mu) & O & \nabla_{\!w\,}s(w,y,\mu) & O \\ O & \nabla^2 f(x) + \sum_j y_j \nabla^2 g_j(x) & \nabla g(x) & I \\ -I & \nabla{g}(x)^T & O & O \\ O & \nabla_{\!z\,} t(x,z,\mu) & O & \nabla_{\!z\,} t(x,z,\mu) \end{pmatrix}
\f]
Set
\f[ \begin{gathered} Q=\nabla^2 f(x) + \sum_j y_j \nabla^2 g_j(x) , \\ A=\nabla g(X)^T, \ W_{jj} = \partial s_j/\partial y_j ; \quad Y_{jj} = \partial s_j/\partial w_j ; \quad X_{ii}=\partial{t_i}/{\partial{z_i}}; \ Z_{ii}=\partial{t_i}/{\partial{x_i}} . \end{gathered} \f]
Note that for a positivity constraint \f$g_j(x)\geq0\f$ we have \f$s_j(w_j,y_j,\mu)=w_jy_j-\mu\f$, so \f$W_{jj}=s_{j,y}(w_j,y_j,\mu)=w_j\f$ and  \f$Y_{jj}=s_{j,w}(w_j,y_j,\mu)=y_j\f$.
The system of linear equations is then
\f[ \begin{pmatrix} -I & A & O & O \\ O & Q & A^T & I \\ Y & O & W & O \\ O & Z & O & X \end{pmatrix} \begin{pmatrix}\Delta{w}\\\Delta{x}\\\Delta{y}\\\Delta{z}\end{pmatrix} = - \begin{pmatrix} g(x)-w \\ f(x) + \sum_{j=1}^{m} y_j \nabla_{\!i\,} g(x) \\ s(g(x),y,\mu) \\ t(x,z,\mu) \end{pmatrix} . \f]
This can be symmetrised to give
\f[ \begin{pmatrix} -W \backslash Y & O & -I & O \\ O & Q & A^T & I \\ -I & A & O & O \\ O & I & O & Z \backslash X \end{pmatrix} \begin{pmatrix}\Delta{w}\\\Delta{x}\\\Delta{y}\\\Delta{z}\end{pmatrix} = - \begin{pmatrix} -W \backslash s(g(x),y,\mu) \\ f(x) + \sum_{j=1}^{m} y_j \nabla_{\!i\,} g(x) \\ g(x)-w \\ Z \backslash t(x,z,\mu) \end{pmatrix} . \f]
Eliminating \f$\Delta{w}\f$ using \f$Y\Delta{w}+W\Delta{Y}=-s(w,y,\mu)\f$ gives
\f[ \begin{pmatrix} Q & A^T & I \\ Y A & W & O \\ Z & O & X \end{pmatrix} \begin{pmatrix}\Delta{x}\\\Delta{y}\\\Delta{z}\end{pmatrix} = - \begin{pmatrix} f(x) + \sum_{j=1}^{m} y_j \nabla_{\!i\,} g(x) \\ s(w,y,\mu) + Y (g(x)-w) \\ t(x,z,\mu) \end{pmatrix} . \f]
with symmetric form
\f[ \begin{pmatrix} Q & A^T & I \\ A & Y \backslash W & O \\ I & O & Z\backslash X \end{pmatrix} \begin{pmatrix}\Delta{x}\\\Delta{y}\\\Delta{z}\end{pmatrix} = - \begin{pmatrix} f(x) + \sum_{j=1}^{m} y_j \nabla_{\!i\,} g(x) \\ g(x)-w + Y \backslash s(w,y,\mu) \\ Z \backslash t(x,z,\mu) \end{pmatrix} . \f]
Eliminating \f$\Delta{z}\f$ using \f$Z\Delta{x}+X\Delta{z} = -t(x,z,\mu)\f$ gives the system
\f[ \begin{pmatrix} Q-X\backslash Z & A^T \\ Y A & W \end{pmatrix} \begin{pmatrix}\Delta{x}\\\Delta{y}\end{pmatrix} = - \begin{pmatrix} f(x) + \sum_{j=1}^{m} y_j \nabla_{\!i\,} g(x) - X \backslash t(x,z,\mu) \\ Y (g(x)-w) + s(w,y,\mu)\end{pmatrix} . \f]
with symmetric form
\f[ \begin{pmatrix} Q - X\backslash Z & A^T \\ A & Y \backslash W \end{pmatrix} \begin{pmatrix}\Delta{x}\\\Delta{y}\end{pmatrix} = - \begin{pmatrix} f(x) + \sum_{j=1}^{m} y_j \nabla_{\!i\,} g(x) -X \backslash t(x,z,\mu) \\ g(x)-w + Y \backslash s(w,y,\mu) \end{pmatrix} . \f]
Eliminating \f$\Delta{y}\f$ using \f$YA\Delta{x}+W\Delta{y}=-\bigl(Y(g(x)-w)+s(w,y,\mu)\bigr)\f$ gives the equation
\f[ (Q - A^TW^{-1}YA-X^{-1}Z) \Delta{x} = - \bigl( (\nabla{f}(x)-\sum_{j=1}^{m}y_j \nabla{g_j}(x))-A^TW^{-1}(Y(g(x)-w)+s(w,y,\mu))-X^{-1}t(x,z,\mu)\bigr) . \f]
In particular, the matrix can be formed as long as \f$\partial s_j/\partial y_j \neq 0\f$ and \f$\partial t_i/\partial z_i \neq 0\f$ for all \f$i,j\f$.

Alternatively, rather than introduce slack variables, we introduce \f$w=g(x)\f$ as an intermediate when solving for the Newton step. This gives
\f[ \begin{pmatrix} Q & A^T & I \\ Y A & W & O \\ Z & O & X \end{pmatrix} \begin{pmatrix}\Delta{x}\\\Delta{y}\\\Delta{z}\end{pmatrix} = - \begin{pmatrix} f(x) + \sum_{j=1}^{m} y_j \nabla_{\!i\,} g(x) \\  s(w,y,\mu) \\ t(x,z,\mu) \end{pmatrix}. \f]








\subsection primaldual Primal-dual methods

In a primal-dual method, we introduce dual variables \f$y_j,z_i\f$ corresponding to each inequality constraint \f$g_j({x})\in [\unl{c}_j \!:\!\ovl{c}_j]\f$ and state constraint \f$x_i\in[\unl{d}_i \!:\! \ovl{d}_i]\f$.
We update the primal and dual variables using a Newton-step
\f[ x'=x+\alpha \Delta{x}; \quad y'=y+\alpha\Delta{y}; \quad z'=z+\alpha\Delta{z} \f]
We may also introduce slack variables \f$w_j=g_j(x)\f$ as temporaries when computing a Newton step, but these are not variables of the Newton equation

These methods are most useful in the case that \f$\unl{c}_j < \ovl{c}_j\f$ for all \f$j\f$.
In this case, the feasible set has nonempty interior, and we can effectively find and maintain points \f$x\f$ such that \f$\unl{d}_i < x_i < \ovl{d}_i\f$ and \f$\unl{c}_j < g_j(x) < \ovl{c}_j\f$ for all \f$i,j\f$.
For this reason these methods are called <em>interior-point</em> methods.


\subsection primaldualslack Primal-dual-slack methods

In a primal-dual-slack method, we also introduce slack variables \f$w_j\f$ corresponding to each inequality constraint \f$\unl{c}_j \leq g_j({x})\leq \ovl{c}_j\f$ to form an equality constraint \f$g_j({x})-w_j=0\f$ and bounds \f$\unl{c}_j \leq w_j \leq \ovl{c}_j\f$.
We update the primal, dual and slack variables in the Newton step
\f[ w'=w+\alpha\Delta{w}; \quad x'=x+\alpha \Delta{x}; \quad y'=y+\alpha\Delta{y}; \quad z'=z+\alpha\Delta{z} \f]
The equation \f$g_j(x)=w_j\f$ implies
\f[ \nabla{g_j}(x) \Delta{x} - \Delta{w_j} = -(g_j({x})-w_j) . \f]
After the Newton-like step, the equality \f$g_j(x') - w_j' = 0\f$ does not necessarily hold, unless \f$g_j\f$ is affine, in which case \f$g_i(x')-w_j'=0\f$ up to roundoff error.

We call methods in which the slack is stored and updated by a Newton-like step <em>primal-dual-slack</em> methods, to distinguish them from ordinary primal-dual methods.
They are also called <em>infeasible-interior-point methods</em> since the variables \f$x\f$ do not have to be feasible for the constraints \f$\unl{c}_j\leq g_j(x)\leq \ovl{c}_j\f$, even if \f$\unl{c}_j<\ovl{c}_j\f$.
Instead, the slack variables \f$w_j\f$ satisfy the bounds \f$\unl{c}_j\leq w_j\leq \ovl{c}_j\f$, and the bounds on \f$g(x)\f$ are enforced in the limit by the equality constraints \f$g_j(x)-w_j=0\f$.

In general, enforcing feasibility of strict inequality constraints on variables using a primal-dual-slack method is easier than feasibility of \f$g_j(x+\alpha\Delta{x})\f$.


\subsection split_dual_interior_point Split-dual interior point methods

We can also split the dual variables \f$y_j=\ovl{y}_j-\unl{y}_j\f$ and \f$z_i=\ovl{z}_i-\unl{z}_i\f$, satisfying the conditions
\f[ \begin{gathered} (w_j-\unl{c}_j)\unl{y}_j = \mu, \quad (\ovl{c}_j-w_j)\ovl{y}_j = \mu, \\ (x_i-\unl{d}_i)\unl{z}_i = \mu, \quad (\ovl{d}_i-x_i)\ovl{z}_i = \mu. \end{gathered} \f]
We can solve for \f$\Delta{x}_i\f$ and \f$\Delta{w}_j\f$ as previously, and then use the updates
\f[ \begin{gathered}
\unl{y}_j \Delta{w_j} + (w_j-\unl{c}_j)\Delta{\unl{y}_j} = -((w_j-\unl{c}_j)\unl{y}_j - \mu), \quad -\ovl{y}_j \Delta{w_j} + (\ovl{c}_j-w_j)\Delta{\ovl{y}_j} = -(\ovl{c}_j-w_j)\ovl{y}_j - \mu); \\
\unl{z}_i \Delta{x_i} + (x_i-\unl{d}_i)\Delta{\unl{z}_i} = -((x_i-\unl{d}_i)\unl{z}_i - \mu), \quad -\ovl{z}_i \Delta{x_i} + (\ovl{d}_i-x_i)\Delta{\ovl{z}_i} = -(\ovl{d}_i-x_i)\ovl{z}_i - \mu).
\end{gathered} \f]


\section nonlinear_optimisation_interval_methods Interval Methods for Nonlinear Programming

See Hansen & Walster (1993).

\subsection nonlinear_optimisation_interval_kkt Interval methods for solving the Karush-Kuhn-Tucker conditions

Consider the standard nonlinear optimisation problem with inequality constraints only
\f[ \min f(x) \text{ s.t. } g(x)\leq0 \f]
and look for a solution with \f$x\in X = [\unl{x},\ovl{x}] \f$

The Karush-Kuhn-Tucker optimality conditions are
\f[ \begin{gathered}
      \nabla_i f(x) + \sum_{j=1}^{m} y_j \cdot \nabla_i g_j(x) = 0; \\
      y_j \, g_j(x) = 0; \quad
      y_j \geq 0, \ g_j(x) \leq 0. \end{gathered}
\f]
Applying the one-dimensional Taylor theorem with remainder to each component, we obtain
\f[ F(x+\delta{x},y+\delta{y}) \in F(x,y) + D_xF(X,y)\,\delta{x} + D_yF(X,Y)\,\delta{y} \f]
and can hence use the Jacobian matrix
\f[ \begin{pmatrix} \nabla^2f(X)+y\!\cdot\!\nabla^2g(X) & \nabla g(X) \\ y\cdot \nabla g(X)^T & \mathrm{diag}(g(X)) \end{pmatrix} \f]
Note that the variables \f$y\f$ only enter into the Jacobian in the non-interval form.
Hence even if the interval \f$Y_i\f$ contining the dual variable \f$y_i\f$ is unbounded, the interval Newton step only requires a suitably-chosen point \f$y_i\in Y_i\f$.


\subsection nonlinear_optimisation_feasibility_interval_methods Interval methods for solving the John conditions

Introduce variables \f$w,y,z\f$ and use the following form of the John conditions
\f[ \boxed{\begin{gathered}
    v + \sum_{j=1}^{m} y_j^2 + \sum_{i=1}^{n} z_i^2 = 1; \\
    g_j(x) - w_j = 0; \\
    v \nabla_{\!i\,}{f}(x) + \sum_{j=1}^{m} y_j \nabla_{\!i\,} g_j(x) + z_i = 0; \\
    (w_j-\unl{c}_j) (\ovl{c}_j-w_j) y_j = 0; \\
    (x_i-\unl{d}_i) (\ovl{d}_i-x_i) z_j = 0.
\end{gathered} } \f]



\section nonlinear_infeasibility_tests Infeasibility tests


\subsection primal_dual_infeasibility_test Primal-dual infeasibility tests

Since a feasible solution \f$(w^*,x^*)\f$ must satisfy
\f[ \sum_{j=1}^{m} y_j^* (g_j(x^*)-w_j^*) = 0, \f]
a test of infeasibility is
\f[ y^* \cdot (g([x])-[w]) \not\ni 0 .\f]
This can be most accurately evaluated as
\f[ \boxed{ \displaystyle (y^* \cdot g)([x])- y^*\cdot [w] \not\ni 0 . } \f]
For this reason, we prefer a method which keeps the dual variables \f$y_j\f$, but not the variables \f$z_i\f$.

This yields the basis of an <em>infeasible interior point</em> method.
We keep the constraints on \f$x\f$ and the inequality constraints on \f$w\f$ satisfied, and try to solve the constraints \f$g_j(x)-w_j = 0 \f$.
The method can be applied with or without explicit use of the dual variables \f$y\f$ and \f$z\f$.

For the problem \f$g_j(x)\leq0\f$, \f$\unl{d}_i \leq x_i \leq \ovl{d}_i\f$, there is no solution in a box \f$[x]\f$ if there exists \f$y^*\f$ such that
\f[ y^*_j \cdot g_j([x]) > 0 . \f]

\subsection nonlinear_optimisation_dual_bounds Bounds on the objective function.

Consider the nonlinear programming problem with constraints,
\f[ \min f(x) \text{ s.t. } \unl{c}_j \leq g_j(x)\leq \ovl{c}_j,\ j=1,\ldots,m;\ \unl{d}_i \leq x_i \leq \ovl{d}_i . \f]

Suppose \f$x_0\f$ is any point in \f$D\f$ (not necessarily feasible).
From the linearisations
\f[ \begin{aligned}
    f(x) &= f(x_0) + \nabla f([x]) (x-x_0) \\
         &= f(x_0) + \nabla f(x_0) (x-x_0) + \bigl(\nabla f([x])-\nabla f(x_0)\bigr)(x-x_0) \\[1ex]
    g(x) &= g(x_0) + \nabla g(x_0) (x-x_0) + \bigl(\nabla g([x])-\nabla g(x_0)\bigr) (x-x_0)
\end{aligned} \f]
we can rearrange
\f[  \nabla g(x_0) (x-x_0) = g(x)-g(x_0) - \bigl(\nabla g([x])-\nabla g(x_0)\bigr) (x-x_0) \f]
and substitute
\f[ \begin{aligned}
 f(x) &= f(x_0) - y_0\cdot\nabla g(x_0)(x-x_0) + \bigl(\nabla f(x_0) + y_0\cdot \nabla g(x_0)\bigr)(x-x_0) + \bigl(\nabla f([x])-\nabla f(x_0)\bigr)(x-x_0) \\
      &= f(x_0) - y_0\cdot\bigl(g(x)-g(x_0)\bigr) + \bigl(\nabla f(x_0) + y_0\cdot\nabla g(x_0)\bigr)(x-x_0)
              + \bigl(\nabla f([x])-\nabla f(x_0)\bigr)(x-x_0) + y_0 \cdot \bigl(\nabla g([x])-\nabla g(x_0)\bigr) (x-x_0) \\
         &= f(x_0) - y_0\cdot\bigl(g(x)-g(x_0)\bigr) + \bigl(\nabla f(x_0) + y_0\cdot\nabla g(x_0)\bigr)(x-x_0)
              + \bigl( (\nabla f([x])+y_0\cdot\nabla g([x])) - (\nabla f(x_0)+y_0\cdot\nabla g(x_0)) \bigr) (x-x_0)
\end{aligned}\f]
Using the fact that \f$y_j g_j(x) \in y_j [c_j] \f$ for <em>any</em> feasible \f$x\f$, we obtain
\f[ \boxed{ f(x) \in f(x_0) + y_0\cdot g(x_0) + \bigl(\nabla f(x_0) + y_0\cdot\nabla g(x_0)\bigr)(x-x_0)
              + \bigl( (\nabla f([x])+y_0\cdot\nabla g([x])) - (\nabla f(x_0)+y_0\cdot\nabla g(x_0)) \bigr) (x-x_0)  + y_0 \cdot [c] } \f]
This equation can also be written as
\f[ \boxed{ f(x) \in f(x_0) + y_0\cdot g(x_0) + \bigl( \nabla f(x_0)+y_0\cdot\nabla g(x_0) \bigr) (x-x_0) + \bigl( \nabla^2 f([x])+y_0\cdot\nabla^2 g([x]) \bigr) (x-x_0)^2 + y_0 \cdot [c] }\f]
A particularly useful form is given below. Note that the function \f$\nabla f+y_0\cdot\nabla g\f$ is <em>first</em> computed and simplified, and only then evaluated over the box \f$[x]\f$. Since we can choose \f$ \nabla f(x_0)+y_0\cdot\nabla g(x_0) \approx 0 \f$, this can be used to obtain good bounds on \f$f(x)\f$.
\f[ \boxed{ f(x) \in f(x_0) + y_0\cdot g(x_0) + \bigl( \nabla f+y_0\cdot\nabla g \bigr)[x] (x-x_0) + y_0 \cdot [c]}
\f]
If the constraints are all inequalities \f$g_j(x)\leq0\f$, then \f$y_0\geq0\f$ and we obtain
\f[ \boxed{ f(x) \geq f(x_0) + y_0\cdot g(x_0) + \bigl( \nabla f+y_0\cdot\nabla g \bigr)[x] (x-x_0) }
\f]


\section proving_nonlinear_feasibility Proving feasibility of equality constraints

Consider the problem of proving that the system of equality constraints \f$h(x)=0\f$ has a solution in a box \f$X\f$, where \f$h:\R^n\rightarrow\R^m\f$ and \f$m\leq n\f$.
If \f$m=n\f$, then if \f$Dh\f$ is invertible, we can find a solution using an interval Newton method.
If \f$m<n\f$, we wish to reformulate the problem in \f$m\f$ variables.

Fix \f$x\f$ and let \f$A\f$ be an approximation to \f$Dh(x)\f$. If \f$Dh(x)\f$ has full (row) rank and \f$A\f$ is sufficiently accurate, then \f$AA^T\f$ is invertible; let \f$R\f$ be an approximate inverse. Let \f$S\f$ be a box in \f$\R^m\f$ containing \f$0\f$, and let \f$f(s) = h(x+A^Ts)\f$ for \f$s\in S\f$.
Note \f$df(0) = Dh(x) A^T \approx AA^T\f$. We can use the interval Newton method to prove that \f$f\f$ has a zero in \f$B\f$. Then \f$h\f$ has a zero in \f$x+A^TS\f$.

If \f$X\f$ is a box in \f$\R^n\f$, then \f$RA(X-x)\f$ is a suitable guess for a box in \f$V\f$.



\section nonlinear_programming_algorithm Complete Algorithms for Nonlinear Programming Problems

\subsection nonlinear_feasibility_algorithm Algorithms for nonlinear feasibility problems

In %Ariadne, we are interested in solving feasibility problems where the constraints gradually tighten. This means that a feasable point can be used as in initial seed for iterating a tighter subproblem, and bound on the feasibly set of \f$y\f$ can be used in a tighter subproblem, and a certificate of infeasibility means that no further subproblems need to be considered.

A possible algorithm for solving feasibility problems is as follows:
 - Use a non-rigorous iterative nonlinear programming solver (e.g. an interior point solver) to find \f$x_0,y_0\f$ giving a local solution to the related optimality problem.
 - If \f$y_0\f$ is feasible, repeat for the tighter subproblems, using computed \f$(x_0,y_0)\f$ as a seed for the iterative solver.
 - If \f$y_0\cdot g([x]) \not \in y_0 \cdot [c]\f$, the problem is infeasible in \f$[x]\f$.
 - Reduce \f$[x]\f$ using the constraint \f$y_0\cdot g([x]) \in y_0 \cdot [c]\f$ and hull/box reduce, possibly using the expansion \f$y_0\cdot g([x]) = y_0\cdot \bigl(g(x_0)+\nabla g([x])(x-x_0)\bigr)\f$.
 - If the resulting \f$[x]\f$ is empty, the problem is infeasible.
 - Apply the Krawczyk contractor to the Karush-Kuhn-Tucker optimality conditions \f$  y\cdot \nabla g(x) = 0; \ \sum y_j =1; \ g(x) + t + z = 0; \ x_i z_i=0; \ x_i,z_i\geq 0,\ i=1,\ldots,n\f$.
 - Split domain in two and repeat. The splitting should be done to increase \em linearity of the \f$\nabla g\f$, since linear solvers are very effective. Note that we should repeat from the beginning in case the failure to prove disjointness was due to \f$(x_0,y_0)\f$ being a local maximum.

 - It may be worth contracting \f$[x]\f$ from time to time to improve the iteration speed.
 - The use of \f$y_0\cdot g([x]) \in y_0 \cdot [c]\f$ rather than \f$g_j([x])\in [c_j]\f$ to perform the contraction is to focus attention on the possibly violated constraints.




*/


/*
Newton method results in solving the equation
\f[ \boxed{\displaystyle
    \left(\begin{matrix} Z&0&X\\-A&H&0\\0&A^T&I\\ \end{matrix}\right)
      \left(\begin{matrix} \Delta x \\ \Delta y \\ \Delta z \end{matrix}\right)
    = \left(\begin{matrix} r_x \\ r_y \\ r_z \end{matrix}\right)
    = \left(\begin{matrix} x \circ z \\ \nabla f(y) - x\cdot\nabla g(y) \\ g(y)+z \end{matrix}\right)
 } \f]
where \f$A=A(y)=\nabla g(y)\f$ and \f$H=H(x,y)=\nabla^2 f(y) - x\cdot \nabla^2 g(y)\f$ and \f$D=D(x,z)=\mathrm{diag}(z)^{-1}\mathrm{diag}(x)\f$, so
\f[ \boxed{ \displaystyle\
        A_{ij}(y) = \frac{\partial g_j(y)}{\partial y_i}; \qquad
        H_{ik}(x,y) = \frac{\partial^2 f(y)}{\partial y_i\partial y_k} - \sum_{j} x_j\,\frac{\partial^2 g_j(y)}{\partial y_i\partial y_k}; \qquad
        D_{jj}(x,z) = \frac{x_j}{z_j} .
\ } \f]
Consider the inverse of the matrix
\f[
    \left(\begin{matrix} Z&0&X\\-A&H&0\\0&A^T&I\\ \end{matrix}\right)
        =
    \left(\begin{matrix} Z&0&0\\0&I&0\\0&0&I\\ \end{matrix}\right)
    \left(\begin{matrix} I&0&Z^{-1}X\\-A&H&0\\0&A^T&I\\ \end{matrix}\right)

\f]
We can factorise the original matrix as
\f[
    \left(\begin{matrix} Z&0&X\\-A&H&0\\0&A^T&I\\ \end{matrix}\right)
        =
    \left(\begin{matrix} Z&0&0 \\ 0&I&0 \\ 0&0&I \end{matrix}\right)
    \left(\begin{matrix} I&0&0 \\ -A&I&AZ^{-1}X \\ 0&0&I \end{matrix}\right)
    \left(\begin{matrix} I&0&0 \\ 0&H-AZ^{-1}XA^T&0 \\ 0&0&I \end{matrix}\right)
    \left(\begin{matrix} I&0&0 \\ 0&I&0 \\ 0&A^T&I \end{matrix}\right)
    \left(\begin{matrix} I&0&Z^{-1}X \\ 0&I&0 \\ 0&0&I \end{matrix}\right)
\f]
Writing \f$D=Z^{-1}X\f$ and \f$S=H-AZ^{-1}XA^T=H-ADA^T\f$  gives inverse
\f[
    \left(\begin{matrix} Z&0&X\\-A&H&0\\0&A^T&I\\ \end{matrix}\right)^{-1}
        =
    \left(\begin{matrix} I&0&-D \\ 0&I&0 \\ 0&0&I \end{matrix}\right)
    \left(\begin{matrix} I&0&0 \\ 0&I&0 \\ 0&-A^T&I \end{matrix}\right)
    \left(\begin{matrix} I&0&0 \\ 0&S^{-1}&0 \\ 0&0&I \end{matrix}\right)
    \left(\begin{matrix} I&0&0 \\ A&I&-AD \\ 0&0&I \end{matrix}\right)
    \left(\begin{matrix} Z^{-1}&0&0 \\ 0&I&0 \\ 0&0&I \end{matrix}\right)
\f]
A closed form for the inverse is
\f[ \left(\begin{matrix}
        Z^{-1}+Z^{-1}XA^TS^{-1}AZ^{-1} & Z^{-1}XA^TS^{-1} & -Z^{-1}X-Z^{-1}XA^TS^{-1}AZ^{-1}X\\
        S^{-1}AZ^{-1} & S^{-1} & -S^{-1}AZ^{-1}X \\
        -A^TS^{-1}AZ^{-1} & -A^TS^{-1} & I+A^TS^{-1}AZ^{-1}X \\
    \end{matrix}\right) \f]


If we have upper and lower bounds on the constraints \f$c^l\leq g(y) \leq c^u\f$, then the matrices \f$A\f$ and \f$D\f$ and \f$H\f$ are replaced by
\f[  \widehat{A} = \left(\begin{matrix}A(y)&-A(y)\end{matrix}\right);  \quad
     \widehat{D} = \left(\begin{matrix}D(x^u,z^u)&0\\0&D(x^l,z^l)\end{matrix}\right); \quad
     \widehat{H} = H(x^u-x^l,y) .
\f]

*/
